Here is the data for the first part of the project: project1data.mat, project1data_labels.mat
 
You should be able to load the data into Matlab or Octave with "load project1data".  (There are also mat-file loaders for SciPy and other tools/environments.)  You'll see two variables, X and Y.  Each row of X is an example: 5903 voxels taken from a single fMRI scan.  The scans were processed as discussed in class on Monday: e.g., warping to a common brain shape and selecting voxels from brain regions that are anticipated to be relevant.  Each element of Y is an integer in 1..5, corresponding to a trial type and outcome.  We'll post the mapping to human-readable labels here soon.
 
[AS edit: Here is the mapping of labels to human-readable labels.] The task the subjects are doing is to press a button when a moving bar reaches a line, but not to press the button if they get an early/late stop signal.  The fMRI scan is taken approximately 6 seconds after the time button would have been pressed, to account for the delay between neural activity and the BOLD signal that the fMRI measures.
    1) Early Stop: Successful stop to an early stop signal.
    2) Late Stop: Successful stop to a late stop signal.
    3) Correct Go: Correct button press (within ~500ms) on a trial with no stop signal.
    4) Incorrect Go: Button press on a trial with stop signal.
    5) False Alarm: No button press on a trial with no stop signal.
 
Your task for project part 1 is to build the best possible classifier to predict Y from X.  We'll evaluate your classifier against held-out data.  Your score will depend on your classifier's accuracy: anything over a threshold will get a base amount of credit, and then accuracies above the threshold will gain additional credit.  You may feel free to use any tools you like: existing libraries or ones you implement yourself, or anything in between.
 
As a hint, you will get decent performance with a support vector machine using a Gaussian kernel of radius 3Ã—105 and a hinge weight of C=10, using 1-vs-rest and voting.  This classifier is not optimal, but it is a good starting place to make sure you understand the data and the problem -- we are looking forward to seeing how much you are all able to beat this baseline.
 
Some issues to think about:
you can try other classifiers, other methods of reducing multi-class to binary, other kernels, feature engineering, other parameter values, different regularizations, different normalizations, ...
make sure to use techniques like cross-validation, holdout, or bootstrap within the training data set to avoid fooling yourself about the accuracy of your classifier
 
Project part 1 will be due Wednesday 11/11 (a bit over two weeks).  We'll post more information soon about how to hand in your work.
 
As a reminder, you are allowed to work in teams of up to 2 people -- see @5 for team finding.
 
So, fire up your Matlab/Octave instances, and see what you can do!
 